{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we are interested in comparing the fluctation in the rate of occurrence of the different clusters with meteorological data, through a regression exercise. The regression will be conducted through three regressorse easily integrated through the ``scikit-learn`` environment, mainly ``RandomForestRegressor``, ``GradientBoostingRegressor`` and ``XGBRegressor``.\n",
    "\n",
    "I have pre-cleaned the meteorological data and gathered it in ``output/meteorological_data.csv``, while the rate of occurrence is stored in ``output/countdf.csv``. You can see how it was computed in the previous notebook, ``2. Clustering.ipynb``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "import shap\n",
    "\n",
    "np.random.seed(seed=42069)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "countdf = pd.read_csv('output/countdf.csv', index_col=0)\n",
    "countdf.index = pd.to_datetime(countdf.index)\n",
    "\n",
    "meteo = pd.read_csv('output/meteorological_data.csv', index_col=0)\n",
    "meteo.index = pd.to_datetime(meteo.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have highlighted three consecutive sequences with valid data for our seismic and meteorological data. The workflow for this work is divided in the following steps:\n",
    "\n",
    " 1. divide the time series in consecutive, valid sequences,\n",
    " 2. transform the target variable through a log<sub>10</sub> transform and a smoothing window,\n",
    " 3. compute statistical moments of the meteorological variables through a smoothing window, \n",
    " 4. split the data in training and testing sets with respect to the temporal nature of the problem,\n",
    " 5. perform a regression with the different regressors such that ``cluster occurence = f(meteorological variables)``, and\n",
    " 6. look at the importance of the variables through shapley values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# those are the three, long, sequences with valid data\n",
    "sequences = ('2020-01-01 0:00:00', '2020-10-06 20:00:00'), ('2021-01-29 01:15:00', '2021-05-26 11:30:00'), ('2021-06-23 20:00:00', '2021-09-14 06:30:00')\n",
    "\n",
    "# smoothing windows of one hour, one day and one week\n",
    "windows = [4, 96, 672]  \n",
    "window_labels = ['1h', '1 day', '1 wk.']\n",
    "\n",
    "# we define a smoothing window for the count of one day, this can be easily changed\n",
    "smooth_window = 48\n",
    "\n",
    "dt = countdf.index[1] - countdf.index[0]\n",
    "x_sequences = []\n",
    "y_sequences_smooth = []\n",
    "y_sequences_raw = []\n",
    "\n",
    "# we slice the data in our three consecutive sequences and transform the rate of occurence as we go\n",
    "for t1, t2 in sequences:\n",
    "    \n",
    "    # we slice the data we're interested in\n",
    "    y = countdf[(t1 < countdf.index) & (countdf.index < t2)]\n",
    "    \n",
    "    # we apply a few transforms, keep the true and smoothed data\n",
    "    ytrue = y.copy()\n",
    "    y = y.rolling(window=smooth_window).mean()\n",
    "    y = y.dropna(axis=0)\n",
    "    y = np.log(y + 1)\n",
    "\n",
    "    # we use pandas' indexing, making sure that we go back far enough for computing the longest smoothing window\n",
    "    valid_times = pd.date_range(y.index[0] - max(windows) * dt, y.index[-1], freq='15min')\n",
    "    x = meteo.loc[valid_times]\n",
    "    x_sequences.append(x)\n",
    "    y_sequences_smooth.append(y)\n",
    "    y_sequences_raw.append(ytrue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we computing the moving statistical moments for the meteorological data\n",
    "nseq = len(x_sequences)\n",
    "\n",
    "dfs = []\n",
    "\n",
    "if window_labels is None:\n",
    "    window_labels = windows\n",
    "\n",
    "for i in range(nseq):\n",
    "    x, y = x_sequences[i], y_sequences_smooth[i]\n",
    "    df = pd.DataFrame(index=y.index)\n",
    "\n",
    "    for j, w in enumerate(windows):\n",
    "        roll = x.rolling(window=w)\n",
    "\n",
    "        maximum = roll.max()\n",
    "        minimum = roll.min()\n",
    "        avg = roll.mean()\n",
    "        std = roll.std()  \n",
    "\n",
    "        avg.columns = avg.columns + f', mean {window_labels[j]}'\n",
    "        std.columns = std.columns + f', std. {window_labels[j]}'\n",
    "\n",
    "        df = df.join((avg, std,), how='inner')\n",
    "\n",
    "    df = df.dropna(axis=1, how='any')\n",
    "    dfs.append(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell does a few things, defining helper functions to split the data in trainining and testing sets through ``get_seasonal_train_test_indices``, as well as fitting a given regressor according to those indices through ``fit_clf_seasonally``. The function ``df_dict_to_excel`` saves a dictionary of dataframes in an excel file, where every sheet is a dataframe. We also define the dictionaries containing the parameters of every regressor acquired through a gridsearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_seasonal_train_test_indices(X, nsplits=5, train_len=96 * 7 * 4, test_len=96 * 7):\n",
    "    train_ii = []\n",
    "    test_ii = []\n",
    "\n",
    "    timesplit = np.floor(len(X) / (train_len + test_len)).astype(int)\n",
    "\n",
    "    t0 = 0\n",
    "    for i in range(timesplit):\n",
    "        t1 = t0 + train_len\n",
    "        t2 = t1 + test_len\n",
    "\n",
    "        if i == timesplit - 1:\n",
    "            train_ii.append(np.hstack((np.arange(t0, t1), np.arange(t2, len(X)))))\n",
    "        else:\n",
    "            train_ii.append(np.arange(t0, t1))\n",
    "\n",
    "        test_ii.append(np.arange(t1, t2))\n",
    "        t0 = t2\n",
    "\n",
    "    if nsplits == 1:\n",
    "        return np.hstack(train_ii), np.hstack(test_ii)\n",
    "    else:\n",
    "\n",
    "        from sklearn.utils import shuffle\n",
    "        train_ii = shuffle(np.hstack(train_ii))\n",
    "        test_ii = shuffle(np.hstack(test_ii))\n",
    "\n",
    "        return zip(np.array_split(train_ii, nsplits), np.array_split(test_ii, nsplits))\n",
    "    \n",
    "def fit_clf_seasonally(clf, X, y, nsplits=5, train_len=96 * 7 * 4, test_len=96 * 7):\n",
    "    kfold = get_seasonal_train_test_indices(X, nsplits=nsplits, train_len=train_len, test_len=test_len)\n",
    "    indices = [i for i in kfold]\n",
    "\n",
    "    scores = np.zeros((nsplits, 2))\n",
    "\n",
    "    from sklearn.base import clone\n",
    "\n",
    "    classifiers = np.zeros(nsplits, dtype='object')\n",
    "    for i in range(nsplits):\n",
    "        classifiers[i] = clone(clf)\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(indices):\n",
    "\n",
    "        clf = classifiers[i]\n",
    "\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        train_score = clf.score(X_train, y_train)\n",
    "        test_score = clf.score(X_test, y_test)\n",
    "\n",
    "        scores[i, 0] = train_score\n",
    "        scores[i, 1] = test_score\n",
    "\n",
    "        explainer = shap.TreeExplainer(clf)\n",
    "        clf.shap_values = explainer.shap_values(X_train, y_train)\n",
    "\n",
    "        clf.X_train = X[train_index]\n",
    "\n",
    "    train_index = np.hstack([i[0] for i in indices])\n",
    "    test_index = np.hstack([i[1] for i in indices])\n",
    "\n",
    "    return classifiers, scores, train_index, test_index\n",
    "\n",
    "def df_dict_to_excel(df_dict, path, header=True, index=True):\n",
    "\n",
    "    writer = pd.ExcelWriter(path, engine='xlsxwriter')\n",
    "\n",
    "    for tab_name, df in df_dict.items():\n",
    "        df.to_excel(writer, sheet_name=tab_name, header=header, index=index)\n",
    "    \n",
    "    wb = writer.book\n",
    "    format = wb.add_format({'num_format': '0.00'})\n",
    "    \n",
    "    for ws in wb.worksheets():\n",
    "        df = df_dict[ws.name]\n",
    "        ws.set_column(1, len(df.columns) + 1, cell_format=format)\n",
    "        length_list = [max([len(i) for i in df.index])] + [len(x) for x in df.columns]\n",
    "        for i, width in enumerate(length_list):\n",
    "            ws.set_column(i, i, width)\n",
    "            ws.conditional_format(1, i, len(df) + 1, i, {'type': '3_color_scale'})\n",
    "\n",
    "    #writer._save()\n",
    "    writer.close()\n",
    "    \n",
    "rf = {'max_depth': 30,\n",
    "        'min_samples_leaf': 0.05,\n",
    "        'min_samples_split': 0.05,\n",
    "        'n_estimators': 250,\n",
    "        'max_features': 'sqrt'}\n",
    "\n",
    "gb = {'learning_rate': 0.05,\n",
    "        'max_depth': 10,\n",
    "        'min_samples_leaf': 0.05,\n",
    "        'min_samples_split': 0.005,\n",
    "        'n_estimators': 250,\n",
    "        'max_features': 'sqrt',\n",
    "        'subsample': 0.6}\n",
    "\n",
    "xgb = {'gamma': 0.5,\n",
    "        'max_depth': 15,\n",
    "        'min_child_weight': 5,\n",
    "        'n_estimators': 250,\n",
    "        'subsample': 0.3,\n",
    "        'learning_rate': 0.05}\n",
    "    \n",
    "models = [GradientBoostingRegressor(**gb), XGBRegressor(**xgb), RandomForestRegressor(**rf)]\n",
    "                                    \n",
    "name_dict = {'RandomForestRegressor': 'Random forests',\n",
    "             'GradientBoostingRegressor': 'Gradient boosting',\n",
    "             'XGBRegressor': 'XGBoost'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7776\n",
      "11272\n",
      "7913\n"
     ]
    }
   ],
   "source": [
    "for df in y_sequences_raw:\n",
    "    print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training cluster 0\n",
      "Training Gradient boosting\n",
      "TRAIN\n",
      "average: 0.74, maximum: 0.75, minimum: 0.74, std: 0.01\n",
      "TEST\n",
      "average: 0.19, maximum: 0.25, minimum: 0.10, std: 0.06\n",
      "\n",
      "\n",
      "Training XGBoost\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN\n",
      "average: 0.74, maximum: 0.75, minimum: 0.72, std: 0.01\n",
      "TEST\n",
      "average: 0.29, maximum: 0.40, minimum: 0.25, std: 0.05\n",
      "\n",
      "\n",
      "Training Random forests\n",
      "TRAIN\n",
      "average: 0.33, maximum: 0.34, minimum: 0.32, std: 0.01\n",
      "TEST\n",
      "average: 0.16, maximum: 0.18, minimum: 0.14, std: 0.02\n",
      "\n",
      "\n",
      "Training cluster 1\n",
      "Training Gradient boosting\n",
      "TRAIN\n",
      "average: 0.86, maximum: 0.86, minimum: 0.86, std: 0.00\n",
      "TEST\n",
      "average: 0.49, maximum: 0.56, minimum: 0.45, std: 0.04\n",
      "\n",
      "\n",
      "Training XGBoost\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN\n",
      "average: 0.89, maximum: 0.89, minimum: 0.88, std: 0.00\n",
      "TEST\n",
      "average: 0.47, maximum: 0.52, minimum: 0.40, std: 0.04\n",
      "\n",
      "\n",
      "Training Random forests\n",
      "TRAIN\n",
      "average: 0.61, maximum: 0.62, minimum: 0.59, std: 0.01\n",
      "TEST\n",
      "average: 0.39, maximum: 0.44, minimum: 0.34, std: 0.04\n",
      "\n",
      "\n",
      "Training cluster 2\n",
      "Training Gradient boosting\n",
      "TRAIN\n",
      "average: 0.74, maximum: 0.75, minimum: 0.73, std: 0.01\n",
      "TEST\n",
      "average: 0.37, maximum: 0.41, minimum: 0.33, std: 0.03\n",
      "\n",
      "\n",
      "Training XGBoost\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN\n",
      "average: 0.84, maximum: 0.85, minimum: 0.84, std: 0.00\n",
      "TEST\n",
      "average: 0.37, maximum: 0.40, minimum: 0.34, std: 0.02\n",
      "\n",
      "\n",
      "Training Random forests\n",
      "TRAIN\n",
      "average: 0.40, maximum: 0.41, minimum: 0.39, std: 0.00\n",
      "TEST\n",
      "average: 0.26, maximum: 0.28, minimum: 0.22, std: 0.02\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_scores(scores):\n",
    "    print(f'average: {scores.mean():.2f}, maximum: {scores.max():.2f}, '\n",
    "          f'minimum: {scores.min():.2f}, std: {scores.std():.2f}')\n",
    "    \n",
    "nclusters = countdf.shape[1]\n",
    "\n",
    "train_scores = np.zeros((nclusters, len(models)))\n",
    "test_scores = np.zeros((nclusters, len(models)))\n",
    "\n",
    "df_dict = {}\n",
    "\n",
    "preds = np.zeros((len(models), nclusters, len(dfs)), dtype='object')\n",
    "\n",
    "X = pd.concat(dfs)\n",
    "y = pd.concat(y_sequences_smooth)\n",
    "\n",
    "for c in range(nclusters):\n",
    "    print(f'Training cluster {c}')\n",
    "    suby = y[f'CLUSTER_{c}']\n",
    "\n",
    "    dataframes = []\n",
    "    for i, clf in enumerate(models):\n",
    "        tag = clf.__class__.__name__\n",
    "        name = name_dict[tag]\n",
    "        title = f'{name}, cluster {c}'\n",
    "        print(f'Training {name}')\n",
    "\n",
    "        clfs, scores, train_index, test_index = fit_clf_seasonally(clf, X.values, suby.values,\n",
    "                                                                    train_len=96 * 7 * 2, test_len=96 * 3)\n",
    "        print('TRAIN')\n",
    "        print_scores(scores[:, 0])\n",
    "        print('TEST')\n",
    "        print_scores(scores[:, 1])\n",
    "        print()\n",
    "\n",
    "        train_scores[c, i] = scores[:, 0].mean()\n",
    "        test_scores[c, i] = scores[:, 1].mean()\n",
    "\n",
    "        coefs = np.vstack([clf.shap_values for clf in clfs])\n",
    "\n",
    "        means = np.stack([clf.shap_values.mean(axis=0) for clf in clfs])\n",
    "        order = np.argsort(np.abs(means.mean(axis=0)))[::-1]\n",
    "        s = pd.Series(np.abs(means.mean(axis=0))[order], index=X.columns[order], name=name)\n",
    "        s.name = name\n",
    "        dataframes.append(s.to_frame())\n",
    "        print()\n",
    "\n",
    "    df_dict[f'Cluster {c}'] = pd.concat(dataframes, axis=1)\n",
    "\n",
    "test_scores[test_scores < 0] = 0\n",
    "weights = test_scores/(test_scores.sum(axis=1))[:, None]  # (1/test_scores) / (1/test_scores).sum(axis=1)[:, None]\n",
    "\n",
    "for i, (key, df) in enumerate(df_dict.items()):\n",
    "    x = np.abs(df.iloc[:, :len(models)])\n",
    "    x = (x - x.min()) / (x.max() - x.min())\n",
    "    w = weights[i][None, :]\n",
    "    wmean = np.sum(w * x, axis=1).values[:, None]\n",
    "    df['Valeur absolue moyenne'] = x.mean(axis=1)\n",
    "    df['Écart type valeur absolue'] = x.std(axis=1)\n",
    "    df['Pondération valeur absolue moyenne'] = wmean\n",
    "    df['Pondération écart type valeur absolue'] = np.sqrt((w * (x - wmean) ** 2).sum(axis=1))\n",
    "    df['Maximum valeur absolue'] = x.max(axis=1)\n",
    "    df['Minimum valeur absolue'] = x.min(axis=1)\n",
    "    df.iloc[:, :len(models)] = x\n",
    "    df_dict[key] = df.sort_values('Pondération valeur absolue moyenne', ascending=False)\n",
    "\n",
    "test_score_df = pd.DataFrame(test_scores,\n",
    "                                index=[f'Cluster {i}' for i in range(nclusters)],\n",
    "                                columns=list(df_dict.values())[0].columns[:len(models)])\n",
    "train_score_df = pd.DataFrame(train_scores,\n",
    "                                index=[f'Cluster {i}' for i in range(nclusters)],\n",
    "                                columns=list(df_dict.values())[0].columns[:len(models)])\n",
    "df_dict['Training score'] = train_score_df\n",
    "df_dict['Testing score'] = test_score_df\n",
    "\n",
    "df_dict_to_excel(df_dict, path='outputs/coefficients_ranking_3clusters.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'output\\\\coefficients_ranking_3clusters.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4464/3473473211.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mr'output\\coefficients_ranking_3clusters.xlsx'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdf_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msheet_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mnmax\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\excel\\_base.py\u001b[0m in \u001b[0;36mread_excel\u001b[1;34m(io, sheet_name, header, names, index_col, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, thousands, comment, skipfooter, convert_float, mangle_dupe_cols, storage_options)\u001b[0m\n\u001b[0;32m    362\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    363\u001b[0m         \u001b[0mshould_close\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 364\u001b[1;33m         \u001b[0mio\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    365\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    366\u001b[0m         raise ValueError(\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\excel\\_base.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, path_or_buffer, engine, storage_options)\u001b[0m\n\u001b[0;32m   1189\u001b[0m                 \u001b[0mext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"xls\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1190\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1191\u001b[1;33m                 ext = inspect_excel_format(\n\u001b[0m\u001b[0;32m   1192\u001b[0m                     \u001b[0mcontent_or_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1193\u001b[0m                 )\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\excel\\_base.py\u001b[0m in \u001b[0;36minspect_excel_format\u001b[1;34m(content_or_path, storage_options)\u001b[0m\n\u001b[0;32m   1068\u001b[0m         \u001b[0mcontent_or_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontent_or_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1069\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1070\u001b[1;33m     with get_handle(\n\u001b[0m\u001b[0;32m   1071\u001b[0m         \u001b[0mcontent_or_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1072\u001b[0m     ) as handle:\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    708\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    709\u001b[0m             \u001b[1;31m# Binary mode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 710\u001b[1;33m             \u001b[0mhandle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    711\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    712\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'output\\\\coefficients_ranking_3clusters.xlsx'"
     ]
    }
   ],
   "source": [
    "path = r'output\\coefficients_ranking_3clusters.xlsx'\n",
    "\n",
    "df_dict = pd.read_excel(path, sheet_name=None)\n",
    "\n",
    "nmax = 10\n",
    "y = np.arange(nmax) + 1\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(10, 3))\n",
    "\n",
    "for c in range(3):\n",
    "    dfi = df_dict[f'Cluster {c}']\n",
    "    \n",
    "    variables = dfi.iloc[:nmax, 0]\n",
    "    mean = dfi['Pondération valeur absolue moyenne'].iloc[:nmax]\n",
    "    std = dfi['Pondération écart type valeur absolue'].iloc[:nmax]\n",
    "    \n",
    "    axs[c].barh(y, mean, fc=colors[c], alpha=0.5, ec=colors[c], lw=1, xerr=np.vstack((np.zeros(nmax), std)), ecolor=colors[c], error_kw=dict(alpha=0.5))\n",
    "    \n",
    "    for i in range(nmax):\n",
    "        axs[c].text(0.01, y[i], variables[i], va='center')\n",
    "    axs[c].invert_yaxis()\n",
    "    axs[c].set_yticks(y)\n",
    "    axs[c].set_xlim(0, 1)\n",
    "\n",
    "    axs[c].xaxis.tick_top()\n",
    "    axs[c].set_xticks([0, 0.5, 1])\n",
    "    axs[c].spines[['right', 'bottom']].set_visible(False)\n",
    "annotate_axs(axs, x=0.99, y=0.99)\n",
    "#fig.savefig(savepath + 'variables.pdf', bbox_inches='tight')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
